{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"LSTM_Main Feed All.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZuRnbLnVAdHW","colab_type":"code","outputId":"2d590c01-2ab3-4a5f-edea-e80dc9aaed26","executionInfo":{"status":"ok","timestamp":1585644485849,"user_tz":-480,"elapsed":60733,"user":{"displayName":"Calvin Chan","photoUrl":"","userId":"00867865793887477850"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["!pip install tensorflow==1.14.0"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n","\u001b[K     |█████████████████████████████▊  | 101.3MB 1.9MB/s eta 0:00:05"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aMdh1fMpd-RI","colab_type":"code","outputId":"859ec05d-b9b3-4716-96e3-b3a158356724","executionInfo":{"status":"ok","timestamp":1585708295499,"user_tz":-480,"elapsed":4641,"user":{"displayName":"Calvin Chan","photoUrl":"","userId":"00867865793887477850"}},"colab":{"base_uri":"https://localhost:8080/","height":467}},"source":["\n","\n","from matplotlib import pyplot as plt\n","import cv2\n","import os\n","import numpy as np\n","import torch\n","\n","from keras.utils import np_utils\n","from keras.callbacks import History \n","\n","from datetime import datetime\n","\n","import tensorflow as tf\n","\n","\n","import os\n","from google.colab import drive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ySblhp2H73L1","colab_type":"code","outputId":"f2e7236b-ce50-45a4-84b7-0485f616eaeb","executionInfo":{"status":"ok","timestamp":1585708326273,"user_tz":-480,"elapsed":27691,"user":{"displayName":"Calvin Chan","photoUrl":"","userId":"00867865793887477850"}},"colab":{"base_uri":"https://localhost:8080/","height":135}},"source":["\n","drive.mount('/content/drive')\n","\n","COMP_PATH = \"/content/drive/My Drive/Colab Notebooks/NUS/CS5242 Neural Networks and Deep Learning/Project\"\n","# COMP_PATH = 'C:/Users/moong/Desktop/CS5242 Project/'\n","# COMP_PATH = 'C:\\\\Users\\\\Calvin\\\\Documents\\\\Python Scripts\\\\NUS\\\\CS5242 Neural Networks and Deep Learning\\\\Project'\n","\n","os.chdir(COMP_PATH)\n","os.listdir()\n","\n","tf.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'1.14.0'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"iSN91A5rZePK","colab_type":"code","colab":{}},"source":["def add_video_segment(video_segment, segment_label, all_video_segments, all_video_labels, target_len):\n","    video_len = len(video_segment)\n","    if (video_len > 0) and (segment_label != 0):\n","        if (video_len < target_len):\n","            padded_video_segment = np.zeros((target_len, 400))\n","            padded_video_segment[0:len(video_segment)] = video_segment\n","        elif (video_len > target_len):\n","            # Get the middle part\n","            start_idx = (video_len-target_len)//2\n","            padded_video_segment = video_segment[start_idx:start_idx+target_len]\n","        else:\n","            padded_video_segment = video_segment\n","        all_video_segments.append(padded_video_segment)\n","        all_video_labels.append(segment_label)\n","    return all_video_segments, all_video_labels\n","\n","def get_video_segment_by_len(video_group_list, label_group_list, target_len):\n","    all_video_segments = []\n","    all_video_labels = []\n","    for i, label_group in enumerate(label_group_list):\n","        video_group = video_group_list[i]\n","        video_segment = []\n","        segment_label = label_group_list[0]\n","        \n","        idx = 0\n","        for j, label in enumerate(label_group):\n","            \n","            if (label != segment_label):\n","                all_video_segments, all_video_labels = add_video_segment(\n","                    video_segment, segment_label, all_video_segments, all_video_labels, target_len)\n","                segment_label = label\n","                video_segment = []\n","                idx = 0\n","            video_segment.append(video_group[j])\n","            idx += 1\n","            if idx == target_len:\n","                all_video_segments, all_video_labels = add_video_segment(\n","                    video_segment, segment_label, all_video_segments, all_video_labels, target_len)\n","                video_segment = []\n","                idx = 0\n","\n","        all_video_segments, all_video_labels = add_video_segment(\n","            video_segment, segment_label, all_video_segments, all_video_labels, target_len)\n","    \n","    return np.array(all_video_segments), np.array(all_video_labels)\n","    \n","def load_data(split_load, actions_dict, GT_folder, DATA_folder, datatype = 'training', target_len = 100):\n","    file_ptr = open(split_load, 'r')\n","    content_all = file_ptr.read().split('\\n')[1:-1]\n","    content_all = [x.strip('./data/groundTruth/') + 't' for x in content_all]\n","    all_tasks = ['tea', 'cereals', 'coffee', 'friedegg', 'juice', 'milk', 'sandwich', 'scrambledegg', 'pancake', 'salat']\n","    \n","    if datatype == 'training':\n","        data_breakfast = []\n","        labels_breakfast = []\n","        for content in content_all:\n","        \n","            file_ptr = open( GT_folder + content, 'r')\n","            curr_gt = file_ptr.read().split('\\n')[:-1]\n","            # label_seq, length_seq = get_label_length_seq(curr_gt)\n","\n","            loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n","        \n","            curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n","            label_curr_video = []\n","            for iik in range(len(curr_gt)):\n","                label_curr_video.append( actions_dict[curr_gt[iik]] )\n","         \n","            data_breakfast.append(curr_data)\n","            labels_breakfast.append(label_curr_video)\n","    \n","        # labels_uniq, labels_uniq_loc = get_label_bounds(labels_breakfast)\n","\n","        all_video_segments, all_video_labels = get_video_segment_by_len(data_breakfast, labels_breakfast, target_len)\n","\n","        print(\"Finish Load the Training data and labels!!!\")     \n","        return all_video_segments, all_video_labels\n","\n","        # return  data_breakfast, labels_uniq\n","\n","        # return  data_breakfast, labels_breakfast\n","    if datatype == 'test':\n","        data_breakfast = []\n","        for content in content_all:\n","        \n","            loc_curr_data = DATA_folder + os.path.splitext(content)[0] + '.gz'\n","        \n","            curr_data = np.loadtxt(loc_curr_data, dtype='float32')\n","            \n","            data_breakfast.append(torch.tensor(curr_data,  dtype=torch.float64 ) )\n","    \n","        print(\"Finish Load the Test data!!!\")\n","        return data_breakfast\n","\n","\n","def find_end_idx(y_train, start_idx, target_len):\n","    orig_start_idx = start_idx\n","    curr_label = y_train[start_idx]\n","    # print(curr_label)\n","    counter = 0\n","    # print(\"start_idx:\", start_idx)\n","    # print(\"adfdasfdd:\", target_len)\n","    while counter < target_len and \\\n","          start_idx < len(y_train) and \\\n","          curr_label == y_train[start_idx]:\n","        # print(start_idx)\n","        # print(y_train[start_idx])\n","        start_idx += 1\n","        counter += 1\n","\n","    if counter >= target_len:\n","        return orig_start_idx + target_len\n","    elif start_idx >= len(y_train):\n","        return len(y_train)\n","    else:\n","        return start_idx\n","\n","def pad_x(x, target_len):\n","    padded_x = np.zeros((target_len, 1, 400))\n","    padded_x[0:len(x)] = x\n","    return padded_x\n","\n","def pad_y(y, target_len):\n","    padded_y = np.zeros((target_len, 1))\n","    padded_y[0:len(y)] = y\n","    return padded_y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xsEwDLK79fIx","colab_type":"code","outputId":"c2cfcb27-1856-4ca9-8faa-eca7c8cc2ad9","executionInfo":{"status":"ok","timestamp":1585709008040,"user_tz":-480,"elapsed":2811,"user":{"displayName":"Calvin Chan","photoUrl":"","userId":"00867865793887477850"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from read_datasetBreakfast import read_mapping_dict\n","\n","COMP_PATH = \"/content/drive/My Drive/Colab Notebooks/NUS/CS5242 Neural Networks and Deep Learning/Project\"\n","\n","''' \n","training to load train set\n","test to load test set\n","'''\n","split = 'training'\n","#split = 'test'\n","train_split =  os.path.join(COMP_PATH, 'splits/5 train.split1.bundle') #Train Split\n","# valid_split =  os.path.join(COMP_PATH, 'splits/2 valid.split1.bundle') #Train Split \n","# train_split =  os.path.join(COMP_PATH, 'splits/300 train.split1 - P16_48.bundle') #Train Split\n","# valid_split =  os.path.join(COMP_PATH, 'splits/90 valid.split1 - P49_54.bundle') # Valid Split \n","# train_split =  os.path.join(COMP_PATH, 'splits/900 train.split1 - P16_48.bundle') #Train Split\n","# valid_split =  os.path.join(COMP_PATH, 'splits/150 valid.split1 - P49_54.bundle') # Valid Split \n","# train_split =  os.path.join(COMP_PATH, 'splits/train.split1 - Orig.bundle') #Train Split\n","test_split  =  os.path.join(COMP_PATH, 'splits/test.split1.bundle') #Test Split\n","GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/') #Ground Truth Labels for each training video \n","DATA_folder =  os.path.join(COMP_PATH, 'data/') #Frame I3D features for all videos\n","mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt') \n","\n","segment_len = 1\n","\n","actions_dict = read_mapping_dict(mapping_loc)\n","if  split == 'training':\n","    startTime = datetime.now()\n","    X_data, Y_data = load_data( train_split, actions_dict, GT_folder, DATA_folder, datatype = split, target_len = segment_len) #Get features and labels\n","    # labels_uniq, labels_uniq_loc = get_label_bounds(data_labels)\n","    endTime = datetime.now()\n","    print(\"******** Total Time to load file:\",(endTime - startTime))\n","if  split == 'test':\n","    data_feat = load_data( test_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features only\n","\n","\n","Y_data = np.reshape(Y_data, (Y_data.shape[0], 1))\n","\n","# print(len(data_feat))\n","# print(len(data_labels))\n","# print(data_feat)\n","# print(data_labels)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Finish Load the Training data and labels!!!\n","******** Total Time to load file: 0:00:01.929998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E06zlmckc9Eh","colab_type":"code","colab":{}},"source":["# print(Y.shape)\n","\n","# label_dict = {}\n","# label_len = {}\n","\n","# target_len_tmp = 99999999\n","# start_idx = 0\n","# max_len = 0\n","# end_idx = find_end_idx(Y, start_idx, target_len_tmp)\n","# while start_idx < len(Y):\n","#     this_len = end_idx - start_idx\n","#     if (this_len > max_len):\n","#         max_len = this_len\n","#         print(\"new max len:\", max_len)\n","#     start_idx = end_idx\n","#     if start_idx < len(Y):\n","#         end_idx = find_end_idx(Y, start_idx, target_len_tmp)\n","    \n","#         curr_label = int(Y[start_idx])\n","#         if curr_label in label_dict:\n","#             label_dict[curr_label] += 1\n","#             if this_len > label_len[curr_label]:\n","#                 label_len[curr_label] = this_len\n","#         else:\n","#             label_dict[curr_label] = 1\n","#             label_len[curr_label] = this_len\n","    \n","\n","# print(max_len)\n","# total_occurence = 0\n","# for i, curr_label in enumerate(label_dict):\n","#     print(\"curr_label: {}; occurence: {}; max length: {}\".format(curr_label, label_dict[curr_label], label_len[curr_label]))\n","#     total_occurence += label_dict[curr_label]\n","    \n","# print(\"total_occurence:\", total_occurence)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mkzw_kqy3xMY","colab_type":"code","colab":{}},"source":["# Y = np_utils.to_categorical(Y, num_classes=48)\n","# # print(Y.shape)\n","# Y = np.reshape(Y, (Y.shape[0], 48))\n","\n","# print(X.shape)\n","# print(Y.shape)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpX1338ZRghw","colab_type":"code","colab":{}},"source":["target_len = 3000\n","\n","# n_hidden_nodes = target_len//2\n","n_hidden_nodes = 100\n","n_dense_nodes = 50\n","n_features = 400\n","n_timesteps = target_len\n","n_outputs = 48\n","\n","n_epoch = 1\n","batch_size = 128\n","init_lr = 0.01\n","\n","clipnorm=1.0\n","clipvalue=0.5\n","\n","input_dropout = 0.4\n","recurrent_dropout = 0.4\n","\n","\n","total_num_of_segments = 7068\n","num_display = 50\n","num_of_itr = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OlSunFY-E6E","colab_type":"code","outputId":"a3053c99-5079-484e-8639-b169425bfe6a","executionInfo":{"status":"ok","timestamp":1585709249590,"user_tz":-480,"elapsed":22203,"user":{"displayName":"Calvin Chan","photoUrl":"","userId":"00867865793887477850"}},"colab":{"base_uri":"https://localhost:8080/","height":407}},"source":["from keras import optimizers\n","from keras import regularizers\n","\n","# from keras.models import Model, Sequential\n","# from keras.layers import Dense, Flatten, Dropout, LSTM\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Bidirectional, BatchNormalization, TimeDistributed, Flatten\n","\n","\n","\n","lr = tf.compat.v1.train.exponential_decay(\n","    init_lr,                # Base learning rate.\n","    batch_size,  # Current index into the dataset.\n","    8000,          # Decay step.\n","    0.8,                # Decay rate.\n","    staircase=True)\n","\n","adam = optimizers.Adam(lr=lr, clipnorm=clipnorm, clipvalue=clipvalue)\n","\n","\n","\n","model = Sequential()\n","# model.add(Bidirectional(LSTM(n_hidden_nodes, \n","#                              input_shape=(n_timesteps, n_features), \n","#                              kernel_initializer='random_uniform',\n","#                              recurrent_initializer='random_uniform',\n","#                             #  kernel_regularizer=regularizers.l2(0.01), \n","#                             #  recurrent_regularizer=regularizers.l2(0.01) \n","#                              )))\n","# model.add(Bidirectional(LSTM(n_hidden_nodes, return_sequences=True, input_shape=(n_timesteps, n_features))))\n","model.add(LSTM(n_hidden_nodes, return_sequences=True, input_shape=(n_timesteps, n_features)))\n","# model.add(Bidirectional(LSTM(n_hidden_nodes, return_sequences=True, input_shape=(n_timesteps, n_features), dropout=input_dropout, recurrent_dropout=recurrent_dropout)))\n","model.add(BatchNormalization())\n","# model.add(Dropout(0.2))\n","# model.add(LSTM(20))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.2))\n","model.add(TimeDistributed(Dense(n_dense_nodes, activation='relu')))\n","# model.add(Dropout(0.2))\n","# model.add(Dense(200, activation='relu'))\n","model.add(TimeDistributed(Dense(n_outputs, activation='softmax')))\n","\n","model.compile(loss='sparse_categorical_crossentropy', metrics=['categorical_accuracy'], optimizer=adam)\n","\n","\n","# Split train vs valid data set\n","ratio = 0.9\n","x_train_size = int(X.shape[0]*ratio)\n","x_train = X[:x_train_size]\n","y_train = Y[:x_train_size]\n","x_valid = X[x_train_size:Y.shape[0]]\n","y_valid = Y[x_train_size:Y.shape[0]]\n","\n","print(\"x_train size: \", x_train.shape[0])\n","print(\"x_valid size: \", x_valid.shape[0])\n","\n","\n","history = History()\n","\n","x_train_size = x_train.shape[0]\n","# start_idx = 0\n","# end_idx = start_idx + target_len\n","total_loss = 0\n","total_acc = 0\n","counter = 0\n","\n","\n","for i in range(n_epoch):\n","    total_loss = 0\n","    total_acc = 0\n","    counter = 0\n","\n","    start_idx = 0\n","    end_idx = find_end_idx(y_train, start_idx, target_len)\n","    startTime = datetime.now()\n","    epoch_itr = 1\n","    while True:\n","        \n","        x_train_one_step = x_train[start_idx:end_idx]\n","        y_train_one_step = y_train[start_idx:end_idx]\n","\n","        # print(y_train_one_step.shape)\n","        curr_label = y_train_one_step[0]\n","\n","        x_train_one_step = pad_x(x_train_one_step, target_len)\n","        y_train_one_step = pad_y(y_train_one_step, target_len)\n","\n","        # y_train_one_step = np_utils.to_categorical(y_train_one_step, num_classes=n_outputs)\n","\n","        x_train_one_step = np.reshape(x_train_one_step, (1, x_train_one_step.shape[0], x_train_one_step.shape[2]))\n","        y_train_one_step = np.reshape(y_train_one_step, (1, y_train_one_step.shape[0], y_train_one_step.shape[1]))\n","\n","        history = model.fit(x_train_one_step, y_train_one_step, epochs=1, batch_size=batch_size, verbose=0)\n","\n","        # print(history.history.keys())\n","        # break\n","        train_loss_list = history.history['loss']\n","        train_acc_list  = history.history['categorical_accuracy']\n","\n","        for i, train_loss in enumerate(train_loss_list):\n","            total_loss += train_loss\n","            total_acc += train_acc_list[i]\n","            counter += 1\n","            average_loss = total_loss / counter\n","            average_acc = total_acc / counter\n","\n","        if (epoch_itr % num_of_itr == 0):\n","            local_loss = total_loss/counter\n","            local_acc = total_acc/counter\n","            # print('***** Iteration {}, ===== acc {:.2f}, loss {:.2f}\\n'.format(epoch_itr, local_acc, local_loss))\n","\n","            for i, train_loss in enumerate(train_loss_list):\n","                print('***** Label {}, ===== start_idx {}, end_idx {}'.format(curr_label, start_idx, end_idx))\n","                print('***** Iteration {}, ===== acc {:.3f}, loss {:.3f}'.format(epoch_itr, train_acc_list[i], train_loss))\n","                print('***** Running Average ===== acc {:.3f}, loss {:.3f}\\n'.format(average_acc, average_loss))\n","\n","        epoch_itr += 1\n","        if end_idx >= x_train_size:\n","            break\n","        else:\n","            start_idx = end_idx\n","            end_idx = find_end_idx(y_train, start_idx, target_len)\n"," \n","        # print(start_idx)\n","        # print(end_idx)\n","\n","    endTime = datetime.now()\n","    # print(\"*** Time took: \"+str(endTime-startTime))\n","\n","    average_loss = total_loss / counter\n","    average_acc = total_acc / counter\n","    print('***** Epoch {}, Time took: {}, ===== acc {:.3f}, loss {:.3f}\\n'.format(i, str(endTime-startTime), average_acc, average_loss))\n","    # print('***** Epoch %, Time took: %, ===== acc %.2f, loss %.2f\\n' % (i, str(endTime-startTime), average_acc, average_loss), end='')\n","\n","\n","    # end_idx = start_idx + target_len\n","\n","    print('\\n# Evaluate on validation data')\n","    x_valid_size = x_valid.shape[0]\n","    start_idx = 0\n","    end_idx = find_end_idx(y_valid, start_idx, target_len)\n","    total_valid_loss = 0\n","    total_valid_acc = 0\n","    counter = 0\n","    while True:\n","        x_valid_one_step = x_valid[start_idx:end_idx]\n","        y_valid_one_step = y_valid[start_idx:end_idx]\n","\n","        x_valid_one_step = pad_x(x_valid_one_step, target_len)\n","        y_valid_one_step = pad_y(y_valid_one_step, target_len)\n","\n","        # y_valid_one_step = np_utils.to_categorical(y_valid_one_step, num_classes=n_outputs)\n","\n","        x_valid_one_step = np.reshape(x_valid_one_step, (1, x_valid_one_step.shape[0], x_valid_one_step.shape[2]))\n","        y_valid_one_step = np.reshape(y_valid_one_step, (1, y_valid_one_step.shape[0], y_valid_one_step.shape[1]))\n","\n","        results = model.evaluate(x_valid_one_step, y_valid_one_step, verbose=2)\n","\n","        # print(type(results))\n","        # print(results)\n","        total_valid_loss += results[0]\n","        total_valid_acc += results[1]\n","        counter += 1\n","\n","        # print('Validation loss, Validation acc:', results)\n","\n","        if end_idx >= x_valid_size:\n","            break\n","        else:\n","            start_idx = end_idx\n","            end_idx = find_end_idx(y_valid, start_idx, target_len)\n","\n","    average_valid_loss = total_valid_loss/counter\n","    average_valid_acc = total_valid_acc/counter\n","\n","    print('***** Running Average ===== acc {:.3f}, loss {:.3f}\\n'.format(average_valid_acc, average_valid_loss))\n","    # print('Validation loss, Validation acc:', results)\n","\n","\n","    # print(history)\n","#     print('acc %.2f, loss %.2f' % (acc, loss), end='')\n","\n","    \n","# print('\\n# Evaluate on test data')\n","# x_valid_size = x_valid.shape[0]\n","# start_idx = 0\n","# end_idx = start_idx + target_len\n","# while end_idx <= x_valid_size:\n","#     x_valid_one_step = x_valid[start_idx:end_idx]\n","#     y_valid_one_step = y_valid[start_idx:end_idx]\n","#     x_valid_one_step = np.reshape(x_valid_one_step, (1, x_valid_one_step.shape[0], x_valid_one_step.shape[2]))\n","#     y_valid_one_step = np.reshape(y_valid_one_step, (1, y_valid_one_step.shape[0], y_valid_one_step.shape[1]))\n","\n","#     results = model.evaluate(x_valid_one_step, y_valid_one_step, verbose=2)\n","\n","#     start_idx = end_idx\n","#     end_idx = start_idx + target_len\n","\n","# print('test loss, test acc:', results)\n","\n","model.summary()\n","\n","model.save('LSTM_Main_3000.h5')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["x_train size:  6559\n","x_valid size:  729\n","***** Epoch 0, Time took: 0:00:17.661282, ===== acc 0.694, loss 3.042\n","\n","\n","# Evaluate on validation data\n","***** Running Average ===== acc 0.879, loss 0.722\n","\n","Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_6 (LSTM)                (None, 3000, 100)         200400    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 3000, 100)         400       \n","_________________________________________________________________\n","time_distributed_11 (TimeDis (None, 3000, 50)          5050      \n","_________________________________________________________________\n","time_distributed_12 (TimeDis (None, 3000, 48)          2448      \n","=================================================================\n","Total params: 208,298\n","Trainable params: 208,098\n","Non-trainable params: 200\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eJ5wszy_lerG","colab_type":"code","outputId":"c6b7097c-12f0-498d-9e72-f5ca53b7190f","executionInfo":{"status":"ok","timestamp":1585709417751,"user_tz":-480,"elapsed":10483,"user":{"displayName":"Calvin Chan","photoUrl":"","userId":"00867865793887477850"}},"colab":{"base_uri":"https://localhost:8080/","height":488}},"source":["# from keras import optimizers\n","# from keras import regularizers\n","\n","# # from keras.models import Model, Sequential\n","# # from keras.layers import Dense, Flatten, Dropout, LSTM\n","# from keras.models import Sequential\n","# from keras.layers import Dense, LSTM, Bidirectional, BatchNormalization, TimeDistributed, Flatten\n","\n","\n","\n","# lr = tf.compat.v1.train.exponential_decay(\n","#     init_lr,                # Base learning rate.\n","#     batch_size,  # Current index into the dataset.\n","#     8000,          # Decay step.\n","#     0.8,                # Decay rate.\n","#     staircase=True)\n","\n","# adam = optimizers.Adam(lr=lr, clipnorm=clipnorm, clipvalue=clipvalue)\n","\n","\n","\n","# model = Sequential()\n","# # model.add(Bidirectional(LSTM(n_hidden_nodes, \n","# #                              input_shape=(n_timesteps, n_features), \n","# #                              kernel_initializer='random_uniform',\n","# #                              recurrent_initializer='random_uniform',\n","# #                             #  kernel_regularizer=regularizers.l2(0.01), \n","# #                             #  recurrent_regularizer=regularizers.l2(0.01) \n","# #                              )))\n","# # model.add(Bidirectional(LSTM(n_hidden_nodes, return_sequences=True, input_shape=(n_timesteps, n_features))))\n","# model.add(LSTM(n_hidden_nodes, return_sequences=True, input_shape=(n_timesteps, n_features)))\n","# # model.add(Bidirectional(LSTM(n_hidden_nodes, return_sequences=True, input_shape=(n_timesteps, n_features), dropout=input_dropout, recurrent_dropout=recurrent_dropout)))\n","# model.add(BatchNormalization())\n","# # model.add(Dropout(0.2))\n","# # model.add(LSTM(20))\n","# # model.add(BatchNormalization())\n","# # model.add(Dropout(0.2))\n","# model.add(TimeDistributed(Dense(n_dense_nodes, activation='relu')))\n","# # model.add(Dropout(0.2))\n","# # model.add(Dense(200, activation='relu'))\n","# model.add(TimeDistributed(Dense(n_outputs, activation='softmax')))\n","\n","# model.compile(loss='sparse_categorical_crossentropy', metrics=['categorical_accuracy'], optimizer=adam)\n","\n","\n","# # Split train vs valid data set\n","# ratio = 0.9\n","# x_train_size = int(X_data.shape[0]*ratio)\n","# x_train = X_data[:x_train_size]\n","# y_train = Y_data[:x_train_size]\n","# x_valid = X_data[x_train_size:Y_data.shape[0]]\n","# y_valid = Y_data[x_train_size:Y_data.shape[0]]\n","\n","# print(\"x_train size: \", x_train.shape[0])\n","# print(\"x_valid size: \", x_valid.shape[0])\n","\n","\n","# history = History()\n","\n","\n","# history = model.fit(X_data, Y_data, epochs=5, batch_size=batch_size, validation_split=0.2, verbose=2)\n","\n","\n","\n","\n","\n","# model.summary()\n","\n","# model.save('LSTM_Main_3000.h5')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["x_train size:  6559\n","x_valid size:  729\n","Train on 5830 samples, validate on 1458 samples\n","Epoch 1/5\n"," - 3s - loss: 0.6179 - categorical_accuracy: 0.0000e+00 - val_loss: 14.7286 - val_categorical_accuracy: 0.0000e+00\n","Epoch 2/5\n"," - 1s - loss: 0.2318 - categorical_accuracy: 0.0000e+00 - val_loss: 15.0638 - val_categorical_accuracy: 0.0000e+00\n","Epoch 3/5\n"," - 1s - loss: 0.2327 - categorical_accuracy: 0.0000e+00 - val_loss: 14.7706 - val_categorical_accuracy: 0.0000e+00\n","Epoch 4/5\n"," - 1s - loss: 0.1876 - categorical_accuracy: 0.0000e+00 - val_loss: 14.7609 - val_categorical_accuracy: 0.0000e+00\n","Epoch 5/5\n"," - 1s - loss: 0.2042 - categorical_accuracy: 0.0000e+00 - val_loss: 14.9838 - val_categorical_accuracy: 0.0000e+00\n","Model: \"sequential_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_8 (LSTM)                (None, 1, 100)            200400    \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 1, 100)            400       \n","_________________________________________________________________\n","time_distributed_15 (TimeDis (None, 1, 50)             5050      \n","_________________________________________________________________\n","time_distributed_16 (TimeDis (None, 1, 48)             2448      \n","=================================================================\n","Total params: 208,298\n","Trainable params: 208,098\n","Non-trainable params: 200\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rxBgzxZOnEy4","colab_type":"code","outputId":"4887ad2b-2315-4d36-c0b2-f4576e250cc3","executionInfo":{"status":"ok","timestamp":1585709320394,"user_tz":-480,"elapsed":870,"user":{"displayName":"Calvin Chan","photoUrl":"","userId":"00867865793887477850"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["# target_len = 1\n","\n","# # n_hidden_nodes = target_len//2\n","# n_hidden_nodes = 100\n","# n_dense_nodes = 50\n","# n_features = 400\n","# n_timesteps = target_len\n","# n_outputs = 48\n","\n","# n_epoch = 1\n","# batch_size = 128\n","# init_lr = 0.01\n","\n","# clipnorm=1.0\n","# clipvalue=0.5\n","\n","# input_dropout = 0.4\n","# recurrent_dropout = 0.4\n","\n","\n","# total_num_of_segments = 7068\n","# num_display = 50\n","# num_of_itr = 100\n","\n","# Y_data = np.reshape(Y_data, (Y_data.shape[0],1,1))\n","# print(Y_data.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(7288, 1, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qrMM0x2xAdJO","colab_type":"code","colab":{}},"source":["# a = []\n","# while True:\n","#     a.append(\"1\")"],"execution_count":0,"outputs":[]}]}