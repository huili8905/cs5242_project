{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Load the Training data and labels!!!\n",
      "Finish Load the Training data and labels!!!\n"
     ]
    }
   ],
   "source": [
    "from read_datasetBreakfast import load_data, read_mapping_dict, load_data_cust\n",
    "import os\n",
    "import torch\n",
    "import seaborn as sn\n",
    "\n",
    "COMP_PATH = 'C:\\\\Users\\\\cherl\\\\Desktop\\\\Masters\\\\CS5242 Neural Networks and Deep Learning\\\\Project'\n",
    "''' \n",
    "training to load train set\n",
    "test to load test set\n",
    "'''\n",
    "split = 'training'\n",
    "#split = 'test'\n",
    "train_split =  os.path.join(COMP_PATH, 'splits/train.split1.bundle') #Train Split\n",
    "test_split  =  os.path.join(COMP_PATH, 'splits/test.split1.bundle') #Test Split\n",
    "GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/') #Ground Truth Labels for each training video \n",
    "DATA_folder =  os.path.join(COMP_PATH, 'data/') #Frame I3D features for all videos\n",
    "mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt') \n",
    "\n",
    "actions_dict = read_mapping_dict(mapping_loc)\n",
    "if  split == 'training':\n",
    "    data_feat_train, data_labels = load_data(train_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features and labels\n",
    "    data_feat_train_cust, data_labels_cust = load_data_cust(train_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features and labels\n",
    "if  split == 'test':\n",
    "    data_feat_test = load_data(test_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features only\n",
    "\n",
    "#'''\n",
    "#Write Code Below\n",
    "#Pointers\n",
    "#Need to load the segments.txt file for segments for test videos \n",
    "#Output the CSV in correct format as shown in Evaluation Section\n",
    "#Id corresponds to the segments in order. \n",
    "#Example - 30-150 = Id 0\n",
    "#          150-428 = Id 1\n",
    "#          428-575 = Id 2\n",
    "#Category is the Class of the Predicted Action\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Data: 199\n",
      "Number of Training Data Labels: 199\n",
      "Size of Sample Training Data #0: torch.Size([1595, 400])\n",
      "Testing Data not loaded\n"
     ]
    }
   ],
   "source": [
    "#Print info on the data\n",
    "print(\"Number of Training Data: {}\".format(len(data_feat_train)))\n",
    "print(\"Number of Training Data Labels: {}\".format(len(data_labels)))\n",
    "print(\"Size of Sample Training Data #0: {}\".format(data_feat_train[0].size()))\n",
    "\n",
    "#Testing Data\n",
    "try:\n",
    "    print(\"Number of Test Data: {}\".format(len(data_feat_test)))\n",
    "    print(\"% of Training Data: {}\".format(len(data_feat_test)/(len(data_feat_train)+len(data_feat_test))*100))\n",
    "except NameError:\n",
    "    print(\"Testing Data not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments : 1042\n"
     ]
    }
   ],
   "source": [
    "#Preparing the data by splitting video into segments\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "segments = []\n",
    "segment_labels = []\n",
    "for i, video in enumerate(data_feat_train):\n",
    "    frame_num = 0\n",
    "    while frame_num < len(video):\n",
    "        #Skip SIL frames\n",
    "        if data_labels_cust[i][frame_num] > 0:\n",
    "            start_index = frame_num # start index of segment\n",
    "            end_index = frame_num # end index of segment\n",
    "            start_label = data_labels_cust[i][frame_num] # label of segment\n",
    "            for index in range(start_index + 1, len(video)):\n",
    "                if data_labels_cust[i][index] == start_label:\n",
    "                    frame_num=index + 1\n",
    "                    end_index=index\n",
    "                else:\n",
    "                    frame_num=index + 1\n",
    "                    break\n",
    "            segment = video[start_index:end_index]\n",
    "            segments.append(segment)\n",
    "            segment_label = []\n",
    "            segment_label.append(start_label)\n",
    "            segment_labels.append(segment_label)\n",
    "            #print(segment.size())\n",
    "            #print(\"start {}, end {}\".format(start_index, end_index))\n",
    "        else:\n",
    "            frame_num += 1\n",
    "#print(segment_labels)\n",
    "\n",
    "# Making all the segments equal length\n",
    "standardized_segments = []\n",
    "target_segment_length = 300 # Standard length to convert all segments to. Tune as needed.\n",
    "\n",
    "for segment in segments:\n",
    "    if len(segment) > target_segment_length:\n",
    "        # Truncate segment\n",
    "        standardized_segment = segment[0:target_segment_length]\n",
    "        standardized_segments.append(standardized_segment.float())\n",
    "    elif len(segment) == target_segment_length:\n",
    "        standardized_segment = segment\n",
    "        standardized_segments.append(standardized_segment.float())\n",
    "    else:\n",
    "        # Pad segment\n",
    "        standardized_segment = torch.zeros(target_segment_length, 400)\n",
    "        standardized_segment[0:len(segment)] = segment\n",
    "        standardized_segments.append(standardized_segment.float())\n",
    "    #print(standardized_segment.size())\n",
    "\n",
    "print(\"Number of segments : {}\".format(len(standardized_segments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created\n"
     ]
    }
   ],
   "source": [
    "# Creating data loaders from data\n",
    "batch_size = 10\n",
    "train_valid_split = 500\n",
    "valid_end = 600\n",
    "standardized_segments_tensor = torch.stack(standardized_segments)\n",
    "segment_labels_tensor = torch.tensor(segment_labels)\n",
    "\n",
    "train_data = TensorDataset(standardized_segments_tensor[:train_valid_split], segment_labels_tensor[:train_valid_split])\n",
    "valid_data = TensorDataset(standardized_segments_tensor[train_valid_split:valid_end], segment_labels_tensor[train_valid_split:valid_end])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "print(\"DataLoaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define the neural network modules\n",
    "\n",
    "class mynet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mynet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 10, stride=5).double()\n",
    "        self.pool = nn.MaxPool2d(5, stride=5).double()\n",
    "        self.fc1 = nn.Linear(target_segment_length * 16, 47).double()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class myGruNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(myGruNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    #Initialize the hidden layer\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class myRnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size, n_layers=1):\n",
    "        super(myRnn, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "         \n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(input, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "    \n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.hidden_dim = hidden_dim   # number of hidden nodes in LSTM\n",
    "        self.dropout = dropout_rate\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first = True, dropout=dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input):\n",
    "        #print(\"Input shape : {}\".format(input.shape))\n",
    "        #print(input[0])\n",
    "        #print(input[1])\n",
    "        lstm_out, h = self.lstm(input)\n",
    "        #print(\"LSTM shape : {}\".format(lstm_out.shape))\n",
    "        #lstm_out = lstm_out.contiguous()#.view(-1, self.hidden_dim)\n",
    "        #print(\"LSTM shape contiguous: {}\".format(lstm_out.shape))\n",
    "        \n",
    "        fc_out = self.fc(lstm_out[:,-1:,:])\n",
    "        #print(\"FC shape : {}\".format(fc_out.shape))\n",
    "        \n",
    "        sigmoid_out = self.sigmoid(fc_out)\n",
    "        #sigmoid_out = sigmoid_out.view(batch_size, input.shape[1], -1)\n",
    "        #print(\"Sigmoid out shape : {}\".format(sigmoid_out.shape))\n",
    "        \n",
    "        #sigmoid_last = sigmoid_out[:, -1,:] # Final output of RNN\n",
    "        #print(\"Sigmoid last shape : {}\".format(sigmoid_last.shape))\n",
    "        return sigmoid_out, h\n",
    "    \n",
    "    def init_hidden (self, batch_size, device):         \n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to run the training\n",
    "import numpy as np\n",
    "\n",
    "def cal_acc(probs, target):\n",
    "    # probs: probability that each image is labeled as 1\n",
    "    # target: ground truth label\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argmax(probs, axis=-1)    \n",
    "        acc = torch.sum(prediction == target)\n",
    "        return acc.item() / len(target) * 100\n",
    "\n",
    "def train_one_pass_rnn(network, optim, device, criterion):\n",
    "  # Input: Network and Optimizer\n",
    "  # Output: Averge accuracy , Avergae loss in the pass\n",
    "    network.train()\n",
    "    acc_one_pass = []\n",
    "    loss_one_pass = []\n",
    "    for i, segment in enumerate(standardized_segments):\n",
    "        optim.zero_grad()\n",
    "        X = torch.tensor(segment).unsqueeze(dim=0).float().to(device)\n",
    "        y = torch.ones(X.size()[1])\n",
    "        y = torch.Tensor.new_full(y, y.size(), segment_labels[i][0], dtype=int, device=device, requires_grad=True)\n",
    "        out, h = network(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(loss)\n",
    "\n",
    "def train_one_pass_lstm(network, optim, device, criterion):\n",
    "    # Input: Network and Optimizer\n",
    "    # Output: Averge accuracy , Avergae loss in the pass\n",
    "    network.train()\n",
    "    \n",
    "    h = network.init_hidden(batch_size, device)\n",
    "    loss_one_pass = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.transpose(1,0).squeeze()\n",
    "        h = tuple([each.data for each in h])   \n",
    "        optim.zero_grad()\n",
    "        out, h = network(inputs)\n",
    "        loss = criterion(out.squeeze(), labels)\n",
    "        loss_one_pass.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    #print(np.mean(loss_one_pass))\n",
    "    return np.mean(loss_one_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7605560541152956\n",
      "3.43256010055542\n",
      "3.309905686378479\n",
      "3.2684721183776855\n",
      "3.227519769668579\n",
      "3.2030724763870237\n",
      "3.1711995458602904\n",
      "3.1635423612594606\n",
      "3.1448573160171507\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "#Run the training\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "epochs = 90\n",
    "\n",
    "# Model paramters\n",
    "input_dim = 400 # number of features in each frame\n",
    "hidden_dim = 150\n",
    "output_dim = 47\n",
    "n_layers = 1\n",
    "dropout_rate = 0.25\n",
    "    \n",
    "#net = myGruNet(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
    "#net = myRnn(input_dim, hidden_dim, output_dim).to(device)\n",
    "net = myLSTM(input_dim, hidden_dim, output_dim, n_layers, dropout_rate).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()#nn.BCELoss()\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_pass_lstm(net, optimizer, device, criterion)\n",
    "    if epoch % 10 == 0:\n",
    "        print(train_loss)\n",
    "        \n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3319 Validation Accuracy: 19.0000\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "net.eval()\n",
    "valid_losses = []\n",
    "valid_accuracy = []\n",
    "\n",
    "v_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "for v_inputs, v_labels in valid_loader:\n",
    "    v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
    "    v_h = tuple([each.data for each in v_h])\n",
    "    \n",
    "    v_labels = v_labels.transpose(1,0).squeeze()\n",
    "    v_output, v_h = net(v_inputs)\n",
    "    \n",
    "    v_loss = criterion(v_output.squeeze(), v_labels)\n",
    "    valid_losses.append(v_loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argmax(v_output, axis=2)    \n",
    "        #print(prediction.flatten())\n",
    "        #print(v_labels)\n",
    "        acc = torch.sum(prediction.flatten() == v_labels)\n",
    "        batch_accuracy = acc.item() / len(v_labels) * 100\n",
    "        valid_accuracy.append(batch_accuracy)     \n",
    "        #print(batch_accuracy)\n",
    "        \n",
    "print(\"Validation Loss: {:.4f}\".format(np.mean(valid_losses)), \"Validation Accuracy: {:.4f}\".format(np.mean(valid_accuracy)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
