{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Load the Training data and labels!!!\n",
      "Finish Load the Training data and labels!!!\n"
     ]
    }
   ],
   "source": [
    "from read_datasetBreakfast import load_data, read_mapping_dict, load_data_cust\n",
    "import os\n",
    "import torch\n",
    "import seaborn as sn\n",
    "\n",
    "COMP_PATH = 'C:\\\\Users\\\\cherl\\\\Desktop\\\\Masters\\\\CS5242 Neural Networks and Deep Learning\\\\Project'\n",
    "''' \n",
    "training to load train set\n",
    "test to load test set\n",
    "'''\n",
    "split = 'training'\n",
    "#split = 'test'\n",
    "train_split =  os.path.join(COMP_PATH, 'splits/train.split1.bundle') #Train Split\n",
    "test_split  =  os.path.join(COMP_PATH, 'splits/test.split1.bundle') #Test Split\n",
    "GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/') #Ground Truth Labels for each training video \n",
    "DATA_folder =  os.path.join(COMP_PATH, 'data/') #Frame I3D features for all videos\n",
    "mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt') \n",
    "\n",
    "actions_dict = read_mapping_dict(mapping_loc)\n",
    "if  split == 'training':\n",
    "    data_feat_train, data_labels = load_data(train_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features and labels\n",
    "    data_feat_train_cust, data_labels_cust = load_data_cust(train_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features and labels\n",
    "if  split == 'test':\n",
    "    data_feat_test = load_data(test_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features only\n",
    "\n",
    "#'''\n",
    "#Write Code Below\n",
    "#Pointers\n",
    "#Need to load the segments.txt file for segments for test videos \n",
    "#Output the CSV in correct format as shown in Evaluation Section\n",
    "#Id corresponds to the segments in order. \n",
    "#Example - 30-150 = Id 0\n",
    "#          150-428 = Id 1\n",
    "#          428-575 = Id 2\n",
    "#Category is the Class of the Predicted Action\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Data: 28\n",
      "Number of Training Data Labels: 28\n",
      "Size of Training Data #0: torch.Size([743, 400])\n",
      "743\n",
      "Testing Data not loaded\n"
     ]
    }
   ],
   "source": [
    "#Print info on the data\n",
    "print(\"Number of Training Data: {}\".format(len(data_feat_train)))\n",
    "print(\"Number of Training Data Labels: {}\".format(len(data_labels)))\n",
    "print(\"Size of Training Data #0: {}\".format(data_feat_train[0].size()))\n",
    "#print(data_feat_train[0][0])\n",
    "print(len(data_labels_cust[0]))\n",
    "#Testing Data\n",
    "try:\n",
    "    print(\"Number of Test Data: {}\".format(len(data_feat_test)))\n",
    "    print(\"% of Training Data: {}\".format(len(data_feat_test)/(len(data_feat_train)+len(data_feat_test))*100))\n",
    "except NameError:\n",
    "    print(\"Testing Data not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3]]\n"
     ]
    }
   ],
   "source": [
    "#Preparing the data by splitting video into segments\n",
    "segments = []\n",
    "segment_labels = []\n",
    "for i, video in enumerate(data_feat_train[0:1]):\n",
    "    frame_num = 0\n",
    "    while frame_num < len(video):\n",
    "        #Skip SIL frames\n",
    "        if data_labels_cust[i][frame_num] > 0:\n",
    "            start_index = frame_num # start index of segment\n",
    "            end_index = frame_num # end index of segment\n",
    "            start_label = data_labels_cust[i][frame_num] # label of segment\n",
    "            for index in range(start_index + 1, len(video)):\n",
    "                if data_labels_cust[i][index] == start_label:\n",
    "                    frame_num=index + 1\n",
    "                    end_index=index\n",
    "                else:\n",
    "                    frame_num=index + 1\n",
    "                    break\n",
    "            segment = video[start_index:end_index]\n",
    "            segments.append(segment)\n",
    "            segment_label = []\n",
    "            segment_label.append(start_label)\n",
    "            segment_labels.append(segment_label)\n",
    "            #print(segment.size())\n",
    "            #print(\"start {}, end {}\".format(start_index, end_index))\n",
    "        else:\n",
    "            frame_num += 1\n",
    "print(segment_labels)\n",
    "\n",
    "#Making all the segments equal length\n",
    "standardized_segments = []\n",
    "target_segment_length = 300 # Standard length to convert all segments to. Tune as needed.\n",
    "\n",
    "for segment in segments:\n",
    "    if len(segment) > target_segment_length:\n",
    "        # Truncate segment\n",
    "        standardized_segment = segment[0:target_segment_length]\n",
    "        standardized_segments.append(standardized_segment)\n",
    "    elif len(segment) == target_segment_length:\n",
    "        standardized_segment = segment\n",
    "        standardized_segments.append(standardized_segment)\n",
    "    else:\n",
    "        # Pad segment\n",
    "        standardized_segment = torch.zeros(target_segment_length, 400)\n",
    "        standardized_segment[0:len(segment)] = segment\n",
    "        standardized_segments.append(standardized_segment)\n",
    "    #print(standardized_segment.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define the neural network modules\n",
    "\n",
    "class mynet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mynet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 10, stride=5).double()\n",
    "        self.pool = nn.MaxPool2d(5, stride=5).double()\n",
    "        self.fc1 = nn.Linear(target_segment_length * 16, 47).double()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class myGruNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(myGruNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    #Initialize the hidden layer\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class myRnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size, n_layers=1):\n",
    "        super(myRnn, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "         \n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(input, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-85-4881e12b1b1a>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-85-4881e12b1b1a>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    if i % 1 == 0:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Helper functions to run the training\n",
    "def cal_acc(probs, target):\n",
    "    # probs: probability that each image is labeled as 1\n",
    "    # target: ground truth label\n",
    "    with torch.no_grad():\n",
    "      prediction = torch.argmax(probs, axis=-1)    \n",
    "      acc = torch.sum(prediction == target)\n",
    "    return acc.item() / len(target) * 100\n",
    "\n",
    "def train_one_pass_rnn(network, optim, device, criterion):\n",
    "  # Input: Network and Optimizer\n",
    "  # Output: Averge accuracy , Avergae loss in the pass\n",
    "  network.train()\n",
    "  acc_one_pass = []\n",
    "  loss_one_pass = []\n",
    "  for i, segment in enumerate(standardized_segments):\n",
    "    X = torch.tensor(segment).unsqueeze(dim=0).float().to(device)\n",
    "    y = torch.ones(X.size()[1])\n",
    "    y = torch.Tensor.new_full(y, y.size(), segment_labels[i][0], dtype=int, device=device, requires_grad=True)\n",
    "    out, h = network(X)\n",
    "    print(out)\n",
    "    print(y)\n",
    "    loss = criterion(out, y)\n",
    "    \n",
    "#     for frame in segment:\n",
    "#         X = torch.tensor(frame).float().to(device)\n",
    "#         h = network.init_hidden().to(device)\n",
    "#         y = torch.tensor(segment_labels[i]).to(device)\n",
    "#         out, h = network(X, h)\n",
    "#         #probs = F.softmax(out, dim=1)\n",
    "#         #predy = torch.argmax(probs, dim=1)\n",
    "#         loss = criterion(out, y)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "          print(\"Iter: {:.2f}. \".format(i / len(standardized_segments)),\n",
    "            \"Train Total Loss: {:.2f}. \".format(loss.item()),\n",
    "            \"Train Accuracy: {:.2f}% \".format(cal_acc(out, y))\n",
    "          )\n",
    "    acc_one_pass.append(cal_acc(out, y))\n",
    "    loss_one_pass.append(loss.item())\n",
    "    return sum(acc_one_pass) / len(acc_one_pass), sum(loss_one_pass) / len(loss_one_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cherl\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5887, -0.0030, -0.1219,  ...,  0.5042, -0.5939, -0.3159],\n",
      "        [ 0.4351,  0.2384,  0.0957,  ...,  0.5670, -0.6630,  0.0778],\n",
      "        [ 0.5049,  0.2067, -0.0947,  ...,  0.7911, -0.4287, -0.1147],\n",
      "        ...,\n",
      "        [ 0.6918, -0.8109, -0.1460,  ...,  0.1309,  0.0309,  0.7485],\n",
      "        [ 0.6670, -0.5091, -0.1392,  ..., -0.0219, -0.0778,  0.6554],\n",
      "        [ 0.6574, -0.7828,  0.1567,  ..., -0.0887, -0.2390,  0.5096]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "Iter: 0.00.  Train Total Loss: 4.18.  Train Accuracy: 0.00% \n"
     ]
    }
   ],
   "source": [
    "#Run the training\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Model paramters\n",
    "input_dim = 400\n",
    "hidden_dim = 256\n",
    "output_dim = 47\n",
    "n_layers = 2\n",
    "    \n",
    "#net = myGruNet(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
    "net = myRnn(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "avg_acc, avg_loss = train_one_pass_rnn(net, optimizer, device, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
