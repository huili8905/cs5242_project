{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Load the Training data and labels!!!\n",
      "Finish Load the Training data and labels!!!\n"
     ]
    }
   ],
   "source": [
    "from read_datasetBreakfast import load_data, read_mapping_dict, load_data_cust\n",
    "import os\n",
    "import torch\n",
    "import seaborn as sn\n",
    "\n",
    "COMP_PATH = 'C:\\\\Users\\\\cherl\\\\Desktop\\\\Masters\\\\CS5242 Neural Networks and Deep Learning\\\\Project'\n",
    "''' \n",
    "training to load train set\n",
    "test to load test set\n",
    "'''\n",
    "split = 'training'\n",
    "#split = 'test'\n",
    "train_split =  os.path.join(COMP_PATH, 'splits/train.split1.bundle') #Train Split\n",
    "test_split  =  os.path.join(COMP_PATH, 'splits/test.split1.bundle') #Test Split\n",
    "GT_folder   =  os.path.join(COMP_PATH, 'groundTruth/') #Ground Truth Labels for each training video \n",
    "DATA_folder =  os.path.join(COMP_PATH, 'data/') #Frame I3D features for all videos\n",
    "mapping_loc =  os.path.join(COMP_PATH, 'splits/mapping_bf.txt') \n",
    "\n",
    "actions_dict = read_mapping_dict(mapping_loc)\n",
    "if  split == 'training':\n",
    "    data_feat_train, data_labels = load_data(train_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features and labels\n",
    "    data_feat_train_cust, data_labels_cust = load_data_cust(train_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features and labels\n",
    "if  split == 'test':\n",
    "    data_feat_test = load_data(test_split, actions_dict, GT_folder, DATA_folder, datatype = split) #Get features only\n",
    "\n",
    "#'''\n",
    "#Write Code Below\n",
    "#Pointers\n",
    "#Need to load the segments.txt file for segments for test videos \n",
    "#Output the CSV in correct format as shown in Evaluation Section\n",
    "#Id corresponds to the segments in order. \n",
    "#Example - 30-150 = Id 0\n",
    "#          150-428 = Id 1\n",
    "#          428-575 = Id 2\n",
    "#Category is the Class of the Predicted Action\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Data: 28\n",
      "Number of Training Data Labels: 28\n",
      "Size of Training Data #0: torch.Size([743, 400])\n",
      "743\n",
      "Testing Data not loaded\n"
     ]
    }
   ],
   "source": [
    "#Print info on the data\n",
    "print(\"Number of Training Data: {}\".format(len(data_feat_train)))\n",
    "print(\"Number of Training Data Labels: {}\".format(len(data_labels)))\n",
    "print(\"Size of Training Data #0: {}\".format(data_feat_train[0].size()))\n",
    "#print(data_feat_train[0][0])\n",
    "print(len(data_labels_cust[0]))\n",
    "#Testing Data\n",
    "try:\n",
    "    print(\"Number of Test Data: {}\".format(len(data_feat_test)))\n",
    "    print(\"% of Training Data: {}\".format(len(data_feat_test)/(len(data_feat_train)+len(data_feat_test))*100))\n",
    "except NameError:\n",
    "    print(\"Testing Data not loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, [tensor([[[-6.8024e+00,  6.7101e+00, -2.2984e-01,  ..., -7.2442e+00,\n",
      "          -2.5436e+00, -1.0276e+01],\n",
      "         [-5.3020e+00,  2.4033e+00, -3.0440e+00,  ..., -8.8439e+00,\n",
      "          -3.4250e+00, -9.3487e+00],\n",
      "         [-6.9147e+00,  2.7013e+00, -4.5374e+00,  ..., -6.3670e+00,\n",
      "          -2.0097e+00, -9.5112e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-8.1542e+00,  7.9004e+00, -2.1133e+00,  ..., -6.9232e-01,\n",
      "           1.3961e+00, -5.7870e+00],\n",
      "         [-8.0485e+00,  7.5849e+00, -2.7436e+00,  ..., -5.8487e-01,\n",
      "           1.2655e-01, -6.7010e+00],\n",
      "         [-7.8306e+00,  7.0122e+00, -1.9561e+00,  ..., -7.5800e-01,\n",
      "           1.0764e+00, -7.0850e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-3.7761e+00,  5.3266e+00, -2.2105e+00,  ..., -7.6152e+00,\n",
      "          -6.8176e+00, -1.2816e+01],\n",
      "         [-7.0605e+00,  3.4285e+00,  2.9680e-02,  ..., -8.3752e+00,\n",
      "          -6.9546e+00, -1.4499e+01],\n",
      "         [-9.6465e+00,  4.9902e+00, -1.1506e+00,  ..., -7.5397e+00,\n",
      "          -7.7137e+00, -1.3226e+01],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6000e+01,  9.7705e+00, -3.1099e+00,  ..., -4.8713e+00,\n",
      "          -2.5640e+00, -8.4743e+00],\n",
      "         [-1.6370e+01,  9.1153e+00, -4.1194e+00,  ..., -4.8795e+00,\n",
      "           2.8289e+00, -7.2923e+00],\n",
      "         [-1.3641e+01,  7.4031e+00, -3.4870e+00,  ..., -4.9681e+00,\n",
      "          -3.1617e+00, -5.8497e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-4.7789e+00,  3.6665e+00,  3.6591e+00,  ...,  7.8701e-01,\n",
      "           1.7042e+00, -1.4751e+00],\n",
      "         [-3.7924e+00,  3.0975e+00,  3.4012e+00,  ...,  7.8894e-01,\n",
      "           1.4418e+00, -3.1323e+00],\n",
      "         [-4.0100e+00,  2.4063e+00,  2.8144e+00,  ...,  3.1475e-01,\n",
      "           2.3140e+00, -3.5039e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-8.8925e+00,  3.2837e+00,  1.2749e+00,  ..., -1.7290e+00,\n",
      "          -9.2429e-01, -8.0814e+00],\n",
      "         [-1.0573e+01,  5.8874e+00,  1.7677e+00,  ..., -2.2993e+00,\n",
      "          -1.2364e-01, -8.4775e+00],\n",
      "         [-8.8159e+00,  4.5162e+00, -7.3399e-02,  ..., -2.7991e+00,\n",
      "           1.5541e+00, -6.8033e+00],\n",
      "         ...,\n",
      "         [-4.0559e+00,  4.1811e+00, -2.3807e+00,  ..., -3.5128e-01,\n",
      "           2.2344e+00, -5.7927e+00],\n",
      "         [-5.0575e+00,  5.2818e+00, -1.5637e+00,  ..., -2.7948e+00,\n",
      "           1.0005e-02, -3.4713e+00],\n",
      "         [-5.3847e+00,  4.8802e+00, -1.6251e+00,  ..., -3.7559e+00,\n",
      "          -1.5545e+00, -3.3259e+00]]]), tensor([[33],\n",
      "        [11],\n",
      "        [33],\n",
      "        [24],\n",
      "        [29],\n",
      "        [27],\n",
      "        [33],\n",
      "        [ 2],\n",
      "        [36],\n",
      "        [20]])])\n"
     ]
    }
   ],
   "source": [
    "#Preparing the data by splitting video into segments\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "segments = []\n",
    "segment_labels = []\n",
    "for i, video in enumerate(data_feat_train):\n",
    "    frame_num = 0\n",
    "    while frame_num < len(video):\n",
    "        #Skip SIL frames\n",
    "        if data_labels_cust[i][frame_num] > 0:\n",
    "            start_index = frame_num # start index of segment\n",
    "            end_index = frame_num # end index of segment\n",
    "            start_label = data_labels_cust[i][frame_num] # label of segment\n",
    "            for index in range(start_index + 1, len(video)):\n",
    "                if data_labels_cust[i][index] == start_label:\n",
    "                    frame_num=index + 1\n",
    "                    end_index=index\n",
    "                else:\n",
    "                    frame_num=index + 1\n",
    "                    break\n",
    "            segment = video[start_index:end_index]\n",
    "            segments.append(segment)\n",
    "            segment_label = []\n",
    "            segment_label.append(start_label)\n",
    "            segment_labels.append(segment_label)\n",
    "            #print(segment.size())\n",
    "            #print(\"start {}, end {}\".format(start_index, end_index))\n",
    "        else:\n",
    "            frame_num += 1\n",
    "#print(segment_labels)\n",
    "\n",
    "# Making all the segments equal length\n",
    "standardized_segments = []\n",
    "target_segment_length = 300 # Standard length to convert all segments to. Tune as needed.\n",
    "\n",
    "for segment in segments:\n",
    "    if len(segment) > target_segment_length:\n",
    "        # Truncate segment\n",
    "        standardized_segment = segment[0:target_segment_length]\n",
    "        standardized_segments.append(standardized_segment.float())\n",
    "    elif len(segment) == target_segment_length:\n",
    "        standardized_segment = segment\n",
    "        standardized_segments.append(standardized_segment.float())\n",
    "    else:\n",
    "        # Pad segment\n",
    "        standardized_segment = torch.zeros(target_segment_length, 400)\n",
    "        standardized_segment[0:len(segment)] = segment\n",
    "        standardized_segments.append(standardized_segment.float())\n",
    "    #print(standardized_segment.size())\n",
    "    \n",
    "# Creating data loaders from data\n",
    "batch_size = 10\n",
    "train_valid_split = 40\n",
    "\n",
    "standardized_segments_tensor = torch.stack(standardized_segments)\n",
    "segment_labels_tensor = torch.tensor(segment_labels)\n",
    "\n",
    "train_data = TensorDataset(standardized_segments_tensor[:train_valid_split], segment_labels_tensor[:train_valid_split])\n",
    "valid_data = TensorDataset(standardized_segments_tensor[train_valid_split:], segment_labels_tensor[train_valid_split:])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(next(enumerate(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Define the neural network modules\n",
    "\n",
    "class mynet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mynet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 10, stride=5).double()\n",
    "        self.pool = nn.MaxPool2d(5, stride=5).double()\n",
    "        self.fc1 = nn.Linear(target_segment_length * 16, 47).double()\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class myGruNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super(myGruNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.relu(out[:,-1]))\n",
    "        return out, h\n",
    "    \n",
    "    #Initialize the hidden layer\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return hidden\n",
    "\n",
    "class myRnn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size, n_layers=1):\n",
    "        super(myRnn, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "         \n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(input, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "    \n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.hidden_dim = hidden_dim   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward (self, input):\n",
    "        \n",
    "        lstm_out, h = self.lstm(input)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) \n",
    "        fc_out = self.fc(lstm_out)\n",
    "        sigmoid_out = self.sigmoid(fc_out)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)\n",
    "        sigmoid_last = sigmoid_out[:, -1] # Final output of RNN\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    def init_hidden (self, batch_size, device):         \n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to run the training\n",
    "def cal_acc(probs, target):\n",
    "    # probs: probability that each image is labeled as 1\n",
    "    # target: ground truth label\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argmax(probs, axis=-1)    \n",
    "        acc = torch.sum(prediction == target)\n",
    "        return acc.item() / len(target) * 100\n",
    "\n",
    "def train_one_pass_rnn(network, optim, device, criterion):\n",
    "  # Input: Network and Optimizer\n",
    "  # Output: Averge accuracy , Avergae loss in the pass\n",
    "    network.train()\n",
    "    acc_one_pass = []\n",
    "    loss_one_pass = []\n",
    "    for i, segment in enumerate(standardized_segments):\n",
    "        optim.zero_grad()\n",
    "        X = torch.tensor(segment).unsqueeze(dim=0).float().to(device)\n",
    "        y = torch.ones(X.size()[1])\n",
    "        y = torch.Tensor.new_full(y, y.size(), segment_labels[i][0], dtype=int, device=device, requires_grad=True)\n",
    "        out, h = network(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(loss)\n",
    "\n",
    "def train_one_pass_lstm(network, optim, device, criterion):\n",
    "    # Input: Network and Optimizer\n",
    "    # Output: Averge accuracy , Avergae loss in the pass\n",
    "    network.train()\n",
    "    \n",
    "    h = network.init_hidden(batch_size, device)\n",
    "    acc_one_pass = []\n",
    "    loss_one_pass = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs.to(device)\n",
    "        labels.to(device)\n",
    "        h = tuple([each.data for each in h])   \n",
    "        optim.zero_grad()\n",
    "        out, h = network(inputs)\n",
    "        loss = criterion(out.squeeze(), y.float())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Double for sequence element 1 in sequence argument at position #1 'tensors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-35f399b95630>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mtrain_one_pass_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-4e4906fd57db>\u001b[0m in \u001b[0;36mtrain_one_pass_lstm\u001b[1;34m(network, optim, device, criterion)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0macc_one_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mloss_one_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for sequence element 1 in sequence argument at position #1 'tensors'"
     ]
    }
   ],
   "source": [
    "#Run the training\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# Model paramters\n",
    "input_dim = 400 # number of features in each frame\n",
    "hidden_dim = 256\n",
    "output_dim = 47\n",
    "n_layers = 2\n",
    "    \n",
    "#net = myGruNet(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
    "#net = myRnn(input_dim, hidden_dim, output_dim).to(device)\n",
    "net = myLSTM(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    train_one_pass_lstm(net, optimizer, device, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
